{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RFT Classifier with ExplainerDashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExplainerDashboard is an interactive Dashboard where you can see the effect of different settings on your Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "from emissions.data import load_data, clean_data, split\n",
    "from emissions.trainer import MakeTransformer\n",
    "\n",
    "from explainerdashboard import ClassifierExplainer, ExplainerDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m----------------start loading data----------------\u001b[0m\n",
      "\u001b[34mData loaded: 187503 records\u001b[0m\n",
      "\u001b[32m----------------start cleaning data----------------\u001b[0m\n",
      "\u001b[31m\n",
      "Records in input data: 187503\u001b[0m\n",
      "\u001b[34m\n",
      "Share of Pass and Fail before cleaning:\u001b[0m\n",
      "\u001b[34mFail: 7%\n",
      "Pass: 93%\u001b[0m\n",
      "\u001b[34m\n",
      "Unique vehicles in Fail: 10813\u001b[0m\n",
      "\u001b[34mUnique vehicles in Pass: 84908\u001b[0m\n",
      "\n",
      "Records with missing GVWR: 8125\n",
      "\u001b[31m\n",
      "Records after droping rows where GVWR is < 1000 or missing: 179373\u001b[0m\n",
      "\u001b[31m\n",
      "Records after keeping only the earliest test within a month for each vehicle: 165732\u001b[0m\n",
      "\n",
      "Records where ODOMETER = 0: 796\n",
      "\u001b[31m\n",
      "Records after droping rows where ODOMETER is missing: 164855\u001b[0m\n",
      "\u001b[31m\n",
      "Records after droping rows where MILE_YEAR > 40,000: 163891\u001b[0m\n",
      "\u001b[31m\n",
      "Records in output data:163891\u001b[0m\n",
      "\u001b[34m\n",
      "Share of Pass and Fail after cleaning:\u001b[0m\n",
      "\u001b[34mFail: 7%\n",
      "Pass: 93%\u001b[0m\n",
      "\u001b[34m\n",
      "Unique vehicles in Fail: 10194\u001b[0m\n",
      "\u001b[34mUnique vehicles in Pass: 78573\u001b[0m\n",
      "['VEHICLE_TYPE' 'MODEL_YEAR' 'VEHICLE_AGE' 'MILE_YEAR' 'GVWR'\n",
      " 'ENGINE_SIZE' 'TRANS_TYPE' 'TEST_TYPE' 'RESULT' 'MAKE'\n",
      " 'ENGINE_WEIGHT_RATIO' 'SPORT' 'TEST_SDATE']\n",
      "\u001b[32m----------------data splitted into train test----------------\u001b[0m\n",
      "\u001b[34mShare of Pass and Fail in train set:\u001b[0m\n",
      "\u001b[34mPass: 7%\n",
      "Fail: 93%\u001b[0m\n",
      "\u001b[34mShare of Pass and Fail in test set:\u001b[0m\n",
      "\u001b[34mPass: 7%\n",
      "Fail: 93%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get the data and split\n",
    "df = load_data('../../data/sample201320.csv')\n",
    "df = clean_data(df)\n",
    "X_train, X_test, y_train, y_test = split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "57 make labels each account for less than 1.0% of cars and together account for 9.98% of cars\n",
      "\n",
      "MAKEs don't belong to other: ['bmw', 'buick', 'chevrolet', 'chrysler', 'dodge', 'ford', 'gmc', 'honda', 'hyundai', 'jeep', 'kia', 'lexus', 'mazda', 'mitsubishi', 'nissan', 'pontiac', 'subaru', 'toyota', 'volkswagen']\n",
      "\n",
      "Number of unique makes in train 20\n",
      "\n",
      "Number of unique makes in test 20\n"
     ]
    }
   ],
   "source": [
    "# choose important columns\n",
    "cols = ['MODEL_YEAR','VEHICLE_AGE','MILE_YEAR', 'ENGINE_WEIGHT_RATIO','MAKE']\n",
    "\n",
    "# transform rare MAKE into other\n",
    "mt = MakeTransformer().fit(X_train[cols])\n",
    "print(\"\\nMAKEs don't belong to other:\", mt.makes_keep)\n",
    "X_train_update = mt.transform(X_train[cols])\n",
    "print('\\nNumber of unique makes in train', X_train_update.MAKE.nunique())\n",
    "X_test_update = mt.transform(X_test[cols])\n",
    "print('\\nNumber of unique makes in test', X_test_update.MAKE.nunique())\n",
    "        \n",
    "# transform MAKE into one-hot numeric array\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "MAKE_train = pd.DataFrame(enc.fit_transform(X_train_update[['MAKE']]).toarray())\n",
    "MAKE_train = MAKE_train.add_prefix('MAKE_')\n",
    "MAKE_test = pd.DataFrame(enc.fit_transform(X_test_update[['MAKE']]).toarray())\n",
    "MAKE_test = MAKE_test.add_prefix('MAKE_')\n",
    "\n",
    "# drop MAKE and add the one-hot numeric array to form one new data frame\n",
    "X_train_rel = X_train_update.drop('MAKE',axis=1)\n",
    "X_train_rel.reset_index(drop=True, inplace=True)\n",
    "MAKE_train.reset_index(drop=True, inplace=True)\n",
    "X_train_rel = pd.concat([X_train_rel, MAKE_train],axis=1)\n",
    "X_test_rel = X_test_update.drop('MAKE',axis=1)\n",
    "X_test_rel.reset_index(drop=True, inplace=True)\n",
    "MAKE_test.reset_index(drop=True, inplace=True)\n",
    "X_test_rel = pd.concat([X_test_rel, pd.DataFrame(MAKE_test)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96     30496\n",
      "           1       0.26      0.02      0.04      2283\n",
      "\n",
      "    accuracy                           0.93     32779\n",
      "   macro avg       0.60      0.51      0.50     32779\n",
      "weighted avg       0.88      0.93      0.90     32779\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier based on settings from GridSearch\n",
    "model = RandomForestClassifier(n_estimators=1000,n_jobs=-1,max_depth=30,\n",
    "                              min_samples_leaf=1,min_samples_split=2)\n",
    "model.fit(X_train_rel, y_train)\n",
    "y_pred = model.predict(X_test_rel)\n",
    "tmp = confusion_matrix(y_test,y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected RandomForestClassifier model: Changing class type to RandomForestClassifierExplainer...\n",
      "Note: model_output=='probability', so assuming that raw shap output of RandomForestClassifier is in probability space...\n",
      "Generating self.shap_explainer = shap.TreeExplainer(model)\n"
     ]
    }
   ],
   "source": [
    "# use Explainer Dashboard with less estimators\n",
    "e_model = RandomForestClassifier(n_estimators=10,n_jobs=-1,max_depth=30,\n",
    "                              min_samples_leaf=1,min_samples_split=2)\n",
    "e_model.fit(X_train_rel, y_train)\n",
    "explainer = ClassifierExplainer(e_model, X_test_rel, y_test, n_jobs=-1)\n",
    "explainer.dump(\"RFT_explainer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building ExplainerDashboard..\n",
      "Generating layout...\n",
      "Calculating shap values...\n",
      "Calculating dependencies...\n",
      "Calculating permutation importances (if slow, try setting n_jobs parameter)...\n",
      "Calculating prediction probabilities...\n",
      "Calculating classification_dfs...\n",
      "Calculating predictions...\n",
      "Calculating pr auc curves...\n",
      "Calculating pred_percentiles...\n",
      "Calculating roc auc curves...\n",
      "Calculating confusion matrices...\n",
      "Calculating liftcurve_dfs...\n",
      "Calculating metrics...\n",
      "Calculating ShadowDecTree for each individual decision tree...\n",
      "Reminder: you can store the explainer (including calculated dependencies) with explainer.dump('explainer.joblib') and reload with e.g. ClassifierExplainer.from_file('explainer.joblib')\n",
      "Registering callbacks...\n",
      "Starting ExplainerDashboard inline (terminate it with ExplainerDashboard.terminate(8050))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"800\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1dcc75a3eb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db = ExplainerDashboard(explainer,shap_interaction=False,mode='inline')\n",
    "db.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
